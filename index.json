[{"authors":["admin"],"categories":null,"content":"Hi! I am Ekaterina (or Katja). I am a research engineer focusing on computer vision and deep learning. During my PhD studies I was mainly working on the tasks of image-based 3D reconstruction and tracking, and afterwards spent some time doing research in deep learning for medical image analysis. Recently I have joined NVIDIA as a data scientist for computer vision and video analytics applications.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://katjasrz.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Hi! I am Ekaterina (or Katja). I am a research engineer focusing on computer vision and deep learning. During my PhD studies I was mainly working on the tasks of image-based 3D reconstruction and tracking, and afterwards spent some time doing research in deep learning for medical image analysis. Recently I have joined NVIDIA as a data scientist for computer vision and video analytics applications.","tags":null,"title":"Ekaterina Sirazitdinova","type":"authors"},{"authors":["N Kumar","R Verma","D Anand","Y Zhou","OF Onder","E Tsougenis","H Chen","A Heng","J Li","Z Hu","Y Wang","NA Koohbanani","M Jahanifar","NZ Tajeddin","A Gooya","N Rajpoot","X Ren","S Zhou","Q Wang","D Shen","CK Yang","CH Weng","WH Yu","CY Yeh","S Yang","S Xu","PH Yeung","P Sun","A Mahbod","G Schaefer","I Ellinger","R Ecker","O Smedby","C Wang","B Chidester","TV Ton","MT Tran","J Ma","MN Do","S Graham","QD Vu","JT Kwak","A Gunda","R Chunduri","C Hu","X Zhou","D Lotfi","R Safdari","A Kascenas","A O'Neil","D Eschweiler","J Stegmaier","Y Cui","B Yin","K Chen","X Tian","P Gruening","E Barth","E Arbel","I Remer","A Ben-Dor","E Sirazitdinova","M Kohl","S Braunewell","Y Li","X Xie","L Shen","KD Baksi","MA Khan","J Choo","A Colomer","V Naranjo","L Pei","M Iftekharuddin","K Roy","D Bhattacharjee","A Pedraza","MG Bueno","S Devanathan","S Radhakrishnan","P Koduganty","Z Wu","G Cai","X Liu","A Sethi"],"categories":null,"content":"","date":1571788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571788800,"objectID":"de772fb392248e70b4822d527856b213","permalink":"https://katjasrz.github.io/publication/2019_monuseg/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2019_monuseg/","section":"publication","summary":"Summary of the results of MoNuSeg 2018 Challenge whose objective was to develop generalizable nuclei segmentation techniques in digital pathology.","tags":["Challenge","Segmentation","Histopathology","Deep Learning","Medical"],"title":"A Multi-organ Nucleus Segmentation Challenge","type":"publication"},{"authors":["D Thomas","E Sirazitdinova","A Sugimoto","R Taniguchi"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"ec53ed0cd1c55f1d5338e054e4e96543","permalink":"https://katjasrz.github.io/publication/2019_3dv/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2019_3dv/","section":"publication","summary":"The running average approach has long been perceived as the best choice for fusing depth measurements captured by a consumer-grade RGB-D camera into a global 3D model. This strategy, however, assumes exact correspondences between points in a 3D model and points in the captured RGB-D images. Such assumption does not hold true in many cases because of errors in motion tracking, noise, occlusions, or inconsistent surface sampling during measurements. Accordingly, reconstructed 3D models suffer unpleasant visual artifacts. In this paper, we visit the depth fusion problem from a probabilistic viewpoint and formulate it as a probabilistic optimization using variational message passing in a Bayesian network. Our formulation enables us to fuse depth images robustly, accurately, and fast for high quality RGB-D keyframe creation, even if exact point correspondences are not always available. Our formulation also allows us to smoothly combine depth and color information for further improvements without increasing computational speed. The quantitative and qualitative comparative evaluation on built keyframes of indoor scenes show that our proposed framework achieves promising results for reconstructing accurate 3D models while using low computational power and being robust against misalignment errors without post-processing.","tags":["3D Vision","Kinect Fusion","RGB-D"],"title":"Revisiting Depth Image Fusion with Variational Message Passing","type":"publication"},{"authors":["T Tuna","L Kuhlmann","S Bishti","E Sirazitdinova","T Deserno","S Wolfart"],"categories":null,"content":"","date":1556496000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556496000,"objectID":"332f0ab66dc6c586301b593668fb1123","permalink":"https://katjasrz.github.io/publication/2019_tuna/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2019_tuna/","section":"publication","summary":"Objectives To compare the removal of simulated biofilm at two different implant‐supported restoration designs with various interproximal oral hygiene aids.\nMethods Mandibular models with a missing first molar were fabricated and provided with single implant analogues (centrally or distally placed) and two different crown designs (conventional [CCD] and alternative crown design [ACD]). Occlusion spray was applied to the crowns to simulate artificial biofilm. Thirty participants (dentists, dental hygienists, and laypersons) were equally divided and asked to clean the interproximal areas with five different cleaning devices to further evaluate if there were differences in their cleaning ability. The outcome was measured via standardized photos and the cleaning ratio, representing the cleaned surfaces in relation to the respective crown surface. Statistical analysis was performed by linear mixed‐effects model with fixed effects for cleaning tools, surfaces, crown design and type of participant, and random effects for crowns.\n Results The mean cleaning ratio for the investigated tools and crown designs were (in%) - Super floss 76 ± 13/ACD and 57 ± 14/CCD (highest cleaning efficiency), followed by dental floss 66 ± 13/ACD and 56 ± 15/CCD, interdental brush 55 ± 10/ACD and 45 ± 9/CCD, electric interspace brush 31 ± 10/ACD and 30 ± 1/CCD, microdroplet floss 8 ± 9/ACD and 9 ± 8/CCD. There was evidence of an overall effect of each factor “cleaning tool,” “surface,” “crown design,” and “participant” (p Conclusions ACD allowed more removal of the artificial biofilm than CCD with Super floss, dental floss, and interdental brush. Flossing and interproximal brushing were the most effective cleaning methods. A complete removal of the artificial biofilm could not be achieved in any group.","tags":["Medical","Dental"],"title":"Removal of Simulated Biofilm at Different Implant Crown Designs with Interproximal Oral Hygiene Aids: An In Vitro Study","type":"publication"},{"authors":["E Sirazitdinova","I Pesic","P Schwehn","H Song","M Satzger","M Sattler","D Weingärtner","TM Deserno"],"categories":null,"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"eca7d29a5826dfcec2d1feba6aaf0bf7","permalink":"https://katjasrz.github.io/publication/2018_cacaie/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2018_cacaie/","section":"publication","summary":"A system for fully automatic contact‐less image‐based measurement of volumetric flow rate in urban drainage structures is presented. The hardware includes two original equipment manufacturer cameras and a single‐board computer on which our custom image processing software is running. The value of water discharge depends on the surface velocity, water level and channel's geometry. The level of the flow is estimated as the difference between distances from the camera to the water surface and from the camera to the channel's bottom. Camera‐to‐water distance is recovered automatically using large‐scale stereo‐matching, whereas the distance to the channel's bottom is measured upon installation. Surface velocity is calculated using cross‐correlation template matching. Individual natural particles in the flow are detected and tracked throughout the sequence of images recorded over a fixed time interval. The relative discharge computation error is lower than 1.34% of the theoretical maximal discharge for a given location, which makes our system competitive to commercial components such as ultrasonic flow meters, while using cheaper technologies.","tags":["3D Vision","Tracking"],"title":"Sewer Discharge Estimation by Stereoscopic Imaging and Synchronized Frame Processing","type":"publication"},{"authors":["E Sirazitdinova","I Pesic","P Schwehn","H Song","M Satzger","D Weingärtner","M Sattler","TM Deserno"],"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"4ade5df35335804c60d26eabf30e0ded","permalink":"https://katjasrz.github.io/publication/2017_spie_omabs/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2017_spie_omabs/","section":"publication","summary":"Overflows in urban drainage structures, or sewers, must be prevented on time to avoid their undesirable consequences. An effective monitoring system able to measure volumetric flow in sewers is needed. Existing stateof-the-art technologies are not robust against harsh sewer conditions and, therefore, cause high maintenance expenses. Having the goal of fully automatic, robust and non-contact volumetric flow measurement in sewers, we came up with an original and innovative idea of a vision-based system for volumetric flow monitoring. On the contrast to existing video-based monitoring systems, we introduce a second camera to the setup and exploit stereo-vision aiming of automatic calibration to the real world. Depth of the flow is estimated as the difference between distances from the camera to the water surface and from the camera to the canal’s bottom. Camerato-water distance is recovered automatically using large-scale stereo matching, while the distance to the canal’s bottom is measured once upon installation. Surface velocity is calculated using cross-correlation template matching. Individual natural particles in the flow are detected and tracked throughout the sequence of images recorded over a fixed time interval. Having the water level and the surface velocity estimated and knowing the geometry of the canal we calculate the discharge. The preliminary evaluation has shown that the average error of depth computation was 3 cm, while the average error of surface velocity resulted in 5 cm/s. Due to the experimental design, these errors are rough estimates. At each acquisition session the reference depth value was measured only once, although the variation in volumetric flow and the gradual transitions between the automatically detected values indicated that the actual depth level has varied. We will address this issue in the next experimental session.","tags":["3D Vision","Tracking"],"title":"Stereo Vision for Fully Automatic Volumetric Flow Measurement in Urban Drainage Structures","type":"publication"},{"authors":["R Tatano","E Ehrlich","B Berkels","E Sirazitdinova","T Deserno","UB Fritz"],"categories":null,"content":"","date":1490745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490745600,"objectID":"85a56fe9aa67acafda64a3daae84b172","permalink":"https://katjasrz.github.io/publication/2017_tatano/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2017_tatano/","section":"publication","summary":"Hard tooth tissue demineralisation is an undesirable side effect of orthodontic treatment with fixed appliances. Whereas both clinically and in digital photographs (DP), demineralisations appear as white spot lesions, WSLs appear as dark areas when quantitative light-induced fluorescence (QLF) imaging is used. This study aims at comparing the reproducibility of the detection of decalcified tooth areas in DP and QLF.","tags":["Medical","Dental","Registration"],"title":"Quantitative Light-induced Fluorescence Images and Digital Photographs - Reproducibility of Manually Marked Demineralisations","type":"publication"},{"authors":["E Sirazitdinova","TM Deserno"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485907200,"objectID":"a636222b35023e49565943e55bd2f50d","permalink":"https://katjasrz.github.io/publication/2017_spie_wound/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2017_spie_wound/","section":"publication","summary":"The state-of-the art method of wound assessment is a manual, imprecise and time-consuming procedure. Per- formed by clinicians, it has limited reproducibility and accuracy, large time consumption and high costs. Novel technologies such as laser scanning microscopy, multi-photon microscopy, optical coherence tomography and hyper-spectral imaging, as well as devices relying on the structured light sensors, make accurate wound assessment possible. However, such methods have limitations due to high costs and may lack portability and availability. In this paper, we present a low-cost wound assessment system and architecture for fast and accurate cutaneous wound assessment using inexpensive consumer smartphone devices. Computer vision techniques are applied either on the device or the server to reconstruct wounds in 3D as dense models, which are generated from images taken with a built-in single camera of a smartphone device. The system architecture includes imaging (smartphone), processing (smartphone or PACS) and storage (PACS) devices. It supports tracking over time by alignment of 3D models, color correction using a reference color card placed into the scene and automatic segmentation of wound regions. Using our system, we are able to detect and document quantitative characteristics of chronic wounds, including size, depth, volume, rate of healing, as well as qualitative characteristics as color, presence of necrosis and type of involved tissue.","tags":["3D Vision","Medical","Wound"],"title":"System Design for 3D Wound Imaging Using Low-Cost Mobile Devices","type":"publication"},{"authors":["E Sirazitdinova","SM Jonas","J Lensen","D Kochanov","R Houben","H Slijp","T Deserno"],"categories":null,"content":"","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480550400,"objectID":"231b744a1a4ced650babbf4a7ad31908","permalink":"https://katjasrz.github.io/publication/2016_eurasip/","publishdate":"2016-12-01T00:00:00Z","relpermalink":"/publication/2016_eurasip/","section":"publication","summary":"A novel approach for positioning using smartphones and image processing techniques is developed. Using structure from motion, 3D reconstructions of given tracks are created and stored as sparse point clouds. Query images are matched later to these 3D models. High computational costs of image matching and limited storage require compressing point clouds without loss of positioning performance. In this work, localization is improved and memory and storage requirements are minimized. We assumed that the computational speed and, at the same time, storage requirements benefit from reducing the number of points with appropriate outlier detection. In particular, our hypothesis was that positioning accuracy is maintained while reducing outliers in a reconstructed model. To evaluate the hypothesis, three methods were compared - (i) density-based (Sotoodeh, International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences XXXVI-5, 2006), (ii) connectivity-based (Wang et al. Comput Graph Forum 32(5):207–10, 2013), and (iii) our distance-based approach. In tenfold cross-validation, applied to a pre-reconstructed reference 3D model, localization accuracy was measured. In each new model, the positions of test images were identified and compared to the according positions in the reference model. We observed that outlier removal has a positive impact on matching run-time and storage requirements, while there are no significant differences in the localization error within the methods. That confirmed our initial hypothesis and allows mobile application of image-based positioning.","tags":["3D Vision","Tracking","Navigation"],"title":"Towards Efficient Mobile Image-Guided Navigation Through Removal of Outliers","type":"publication"},{"authors":["B Berkels","T Deserno","EE Ehrlich","UB Fritz","E Sirazitdinova","R Tatano"],"categories":null,"content":"","date":1454284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454284800,"objectID":"fb092a919059ff26e9d50f01c0aed054","permalink":"https://katjasrz.github.io/publication/2016_berkels/","publishdate":"2016-02-01T00:00:00Z","relpermalink":"/publication/2016_berkels/","section":"publication","summary":"Quantitative light-induced fluorescence (QLF) is widely used to assess the damage of a tooth due to decalcification. In digital photographs, decalcification appears as white spot lesions, i.e. white spots on the tooth surface. We propose a novel multimodal registration approach for the matching of digital photographs and QLF images of decalcified teeth. The registration is based on the idea of contour-to-pixel matching. Here, the curve, which represents the shape of the tooth, is extracted from the QLF image using a contour segmentation by binarization and morphological processing. This curve is aligned to the photo with a non-rigid variational registration approach. Thus, the registration problem is formulated as minimization problem with an objective function that consists of a data term and a regularizer for the deformation. To construct the data term, the photo is pointwise classified into tooth and non-tooth regions. Then, the signed distance function of the tooth region allows to measure the mismatch between curve and photo. As regularizer a higher order, linear elastic prior is used. The resulting minimization problem is solved numerically using bilinear Finite Elements for the spatial discretization and the Gauss-Newton algorithm. The evaluation is based on 150 image pairs, where an average of 5 teeth have been captured from 32 subjects. All registrations have been confirmed correctly by a dental expert. The contour-to-pixel methods can directly be used in 3D for surface-to-voxel tasks.","tags":["Medical","Dental","Registration"],"title":"Non-Rigid Contour-to-Pixel Registration of Photographic and Quantitative Light-Induced Fluorescence Imaging of Decalcified Teeth","type":"publication"},{"authors":["SM Jonas","E Sirazitdinova","J Lensen","D Kochanov","H Mayzek","T de Heus","R Houben","H Slijp","T Deserno"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"49a6dd41b5d29c2bf08a8eb087fddedb","permalink":"https://katjasrz.github.io/publication/2015_imago/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/2015_imago/","section":"publication","summary":"Blind and visually impaired persons face many challenges due to isolation, the most important is lacking education due to social immobility. Yet, since the development of the well-known white-red cane, only few advances have been made to increase the mobility of blind people. GPS systems are often used for pedestrian navigation but lack precision, foremost in rural areas where navigation is most often needed. The aim of the IMAGO project is therefore the development of an inexpensive and unobtrusive navigation method for blind and visually impaired persons. Navigation is performed using structure from motion and image-based localization techniques. Route models are created as 3D point clouds through several steps - (i) image acquisition along the routes; (ii) bulk-transfer of images to a server; (iii) feature extraction; (iv) feature matching between images; (v) creation of 3D point cloud with structure from motion. Similarly, the navigation chains seven steps - (a) image acquisition while walking a route; (b) immediate transfer of image to a server; (c) feature extraction; (d) feature matching between image and 3D route model; (e) localization/camera matrix calculation; (f) navigation/calculation of direction based on localization; (g) transfer of direction to user. Current smartphones are used as devices both for recording of routes as well as navigation. Thereby, a high level of dissemination without additional costs is possible, both, within blind people for navigation, as well as seeing people for route creation. Additionally, haptic feedback can be used via a smart cane to reduce auditive feedback. The proposed system yields a high positioning accuracy of 80% of samples being located within 1.6 m. Thus, the system is usable for pedestrian navigation, especially for visually impaired persons.","tags":["3D Vision","Tracking","Navigation"],"title":"IMAGO: Image-Guided Navigation for Visually Impaired People","type":"publication"}]